resource "google_bigquery_dataset" "workflow_dataset" {
  dataset_id                 = var.workflow_dataset
  friendly_name              = "Workflow Dataset"
  description                = "This dataset contains data used by Workflow to orchestrate the data pipeline."
  location                   = var.region
  delete_contents_on_destroy = true
}

resource "google_bigquery_table" "stored_file_attributes" {
  dataset_id  = google_bigquery_dataset.workflow_dataset.dataset_id
  table_id    = "stored_file_attributes"
  description = "Attributes of objects (files) uploaded to the drop zone bucket and managed by the workflow."

  time_partitioning {
    type          = "DAY"
    field         = "time_created"
    expiration_ms = 604800000 # 7 days in milliseconds
  }

  #clustering = ["time_created", "status"]

  schema = <<EOF
[
 {
    "name": "bucket",
    "type": "STRING",
    "description": "The name of the bucket containing the object."
 },
 {
    "name": "name",
    "type": "STRING",
    "description": "The name of the object in the bucket."
 },
 {
    "name": "size",
    "type": "INT64",
    "description": "Size of the object in bytes."
 },
 {
    "name": "time_created",
    "type": "TIMESTAMP",
    "description": "The creation time of the object."
 },
 {
    "name": "status",
    "type": "STRING",
    "description": "Current status of the object in the workflow."
 }
]
EOF
}


resource "google_bigquery_dataset" "pipeline_dataset" {
  dataset_id                 = var.pipeline_dataset
  friendly_name              = "Pipeline Dataset"
  description                = "This dataset contains data processed or generated by the steps of the data pipeline."
  location                   = var.region
  delete_contents_on_destroy = true
}
